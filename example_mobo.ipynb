{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "776d7dd0-74b9-4bf1-85e3-365c3b5cd9d5",
   "metadata": {},
   "source": [
    "# Example MOBO\n",
    "\n",
    "This notebook gives a minimal example of a multi-objective Bayesian optimization (MOBO) algorithm on molecules.\n",
    "See `README.md` for installation instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bcce156-a55f-43fc-b029-7746bcbd29e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kern_gp  # put kern_gp in PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d934cf0b-c49d-453c-bb61-739d0d7bb9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from tdc import Oracle\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5212deee-6091-48c9-bffa-2fbdb82f2386",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "MOBO needs a set of objective functions to optimize.\n",
    "We will try to [simultaneously] maximize the objectives:\n",
    "\n",
    "\\begin{align*}\n",
    "f_1(m) &= -\\textrm{DockingScore}(\\textrm{PPARD}, m) \\\\\n",
    "f_2(m) &= \\textrm{QED}(m) \\\\\n",
    "f_3(m) &= \\textrm{sim}(m, \\textrm{celecoxib})\n",
    "\\end{align*}\n",
    "\n",
    "- $f_1$ is a negative docking score: higher values of $f_1$ indicate stronger binding to the PPARD target.\n",
    "- $f_2$ is the quantatitive estimate of druglikeness. Higher values indicate superfical similarity to previously discovered drugs (on the basis of molecular weight, avoiding a small number of toxic substructures, etc).\n",
    "- $f_3$ is similarity to the known drug molecule celecoxib. This is one of the objectives in the PMO benchmark.\n",
    "\n",
    "Together, these objectives specify selecting molecules which bind to PPARD, are drug-like, and are structurally similar to celecoxib. Note this is just a demo objective; it probably does not correspond to a realistic drug discovery task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef98731d-920d-48d4-9216-1c745eccadb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dockstring dataset\n",
    "from dockstring.dataset import load_dataset\n",
    "DOCKSTRING_DATASET = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b46af5d2-061c-4e47-94c0-858977af05d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"oracles\" for f2 and f3\n",
    "QED_ORACLE = Oracle(\"qed\")\n",
    "CELECOXIB_ORACLE = Oracle(\"celecoxib-rediscovery\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43afd9d9-48b0-401b-9aa6-8577ce351636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_objectives(smiles_list: list[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Given a list of N smiles, return an NxK array A such that\n",
    "    A_{ij} is the jth objective function on the ith SMILES.\n",
    "\n",
    "    NOTE: you might replace this implementation with an alternative\n",
    "    implementation for your objective of interest.\n",
    "\n",
    "    Our specific implementation uses the objective above.\n",
    "    Because it uses the dockstring dataset to look up PPARD values,\n",
    "    it is only defined on SMILES in the dockstring dataset.\n",
    "\n",
    "    Also, be careful of NaN values! Some docking scores might be NaN.\n",
    "    These will need to be dealt with somehow.\n",
    "    \"\"\"\n",
    "    f1 = [- DOCKSTRING_DATASET[\"PPARD\"][s] for s in smiles_list]\n",
    "    f2 = QED_ORACLE(smiles_list)\n",
    "    f3 = CELECOXIB_ORACLE(smiles_list)\n",
    "    out = np.stack([f1, f2, f3])  # 3xN\n",
    "    return out.T  # tranpose, Nx3\n",
    "\n",
    "# Test the function to ensure it outputs the right thing\n",
    "test_inputs = [\n",
    "    'C1=C(C2=C(C=C1O)OC(C(C2=O)=O)C3=CC=C(C(=C3)O)O)O',\n",
    "    'O=S(=O)(N1CCNCCC1)C2=CC=CC=3C2=CC=NC3',\n",
    "    'C=1(N=C(C=2C=NC=CC2)C=CN1)NC=3C=C(NC(C4=CC=C(CN5CCN(CC5)C)C=C4)=O)C=CC3C',\n",
    "    'C1=CC=2C(=CNC2C=C1)C=3C=CN=CC3',\n",
    "]\n",
    "test_outputs = np.asarray(\n",
    "    [\n",
    "        [8.2, .463, 0.139],\n",
    "        [7.1, 0.903, 0.173,], \n",
    "        [10.8, 0.389, 0.201,],\n",
    "        [7.7, 0.633, 0.196,],\n",
    "    ]\n",
    ")\n",
    "actual_outputs = evaluate_objectives(test_inputs)\n",
    "assert np.allclose(\n",
    "    actual_outputs,\n",
    "    test_outputs,\n",
    "    atol=1e-3\n",
    ")\n",
    "del actual_outputs   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6c9128-1696-4d46-b69f-630c90e5f93a",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "At each step of optimization, assume we have a list of molecules for which all objective values have been observed (possibly with noise).\n",
    "We will represent molecules using SMILES strings, and keep a list of such SMILES strings in the variable `known_smiles`.\n",
    "We will store the objective evaluations in an array `known_Y`,\n",
    "where `known_Y[i,j]` is a noisy observation of $f_j$ on the molecule `known_smiles[i]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af96c52e-ccf4-485e-972a-74e4f5522dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_SMILES = list(DOCKSTRING_DATASET[\"PPARD\"].keys())[:10_000]  # 10k SMILES from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90793835-8d7d-415c-983d-0039d721ceb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known SMILES:\n",
      "['C1=C(C2=C(C=C1O)OC(C(C2=O)=O)C3=CC=C(C(=C3)O)O)O',\n",
      " 'O=S(=O)(N1CCNCCC1)C2=CC=CC=3C2=CC=NC3',\n",
      " 'C=1C=C2S/C(/N(CC)C2=CC1OC)=C\\\\C(=O)C',\n",
      " 'C=1(N=C(C=2C=NC=CC2)C=CN1)NC=3C=C(NC(C4=CC=C(CN5CCN(CC5)C)C=C4)=O)C=CC3C',\n",
      " 'C1=CC=2C(=CNC2C=C1)C=3C=CN=CC3',\n",
      " 'N1(C2=C(C(N)=NC=N2)C=N1)C3=CC=CC=C3',\n",
      " 'C1(=C2C(C=CC=C2)=NC=N1)NC3=CC(OC)=CC=C3',\n",
      " 'N1C(N(C(C2=CC=CC=C12)=O)CCN3CCC(CC3)=C(C=4C=CC(=CC4)F)C=5C=CC(=CC5)F)=S',\n",
      " 'C1(O[C@@H](CC(C(=CC([C@H]([C@H](C([C@@H](C[C@@H](C=CC=CC=C([C@H](C[C@H]2O[C@](C(C(N3[C@H]1CCCC3)=O)=O)(O)[C@@H](CC2)C)OC)C)C)C)=O)OC)O)C)C)=O)[C@@H](C[C@H]4C[C@@H](OC)[C@H](O)CC4)C)=O',\n",
      " 'O=C1C=2C=3C(=NNC3C=CC2)C4=C1C=CC=C4']\n",
      "Known Y shape: (10, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 8.2       ,  0.4632294 ,  0.13913043],\n",
       "       [ 7.1       ,  0.90345663,  0.17272727],\n",
       "       [ 6.6       ,  0.77000333,  0.12380952],\n",
       "       [10.8       ,  0.38941616,  0.20134228],\n",
       "       [ 7.7       ,  0.63261731,  0.19587629],\n",
       "       [ 7.3       ,  0.66007647,  0.19191919],\n",
       "       [ 8.9       ,  0.77478383,  0.19811321],\n",
       "       [11.5       ,  0.35317205,  0.1862069 ],\n",
       "       [ 8.4       ,  0.2022053 ,  0.07359307],\n",
       "       [ 8.5       ,  0.49495117,  0.18627451]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known_smiles = ALL_SMILES[:10]  # start with just 10 SMILES known\n",
    "print(\"Known SMILES:\")\n",
    "pprint(known_smiles)\n",
    "\n",
    "known_Y = evaluate_objectives(known_smiles)\n",
    "print(f\"Known Y shape: {known_Y.shape}\")\n",
    "known_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94aac8d-40a9-4016-ab8b-9cee1162f44d",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We will write a model to predict $[f_1,f_2,f_3]$ for arbitrary other molecules $m$,\n",
    "given previous objective function evaluations.\n",
    "We will model each objective independently, and assume Gaussian noise.\n",
    "This means our model's predictive distribution will be a multi-variate Gaussian\n",
    "distribution with no correlations (aka a diagonal covariance matrix):\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}f_1(m) \\\\ f_2(m) \\\\ f_3(m) \\end{bmatrix}\n",
    "\\sim\n",
    "\\mathcal N \\left(\n",
    "\\begin{bmatrix}\\mu_1(m) \\\\ \\mu_2(m) \\\\ \\mu_3(m) \\end{bmatrix},\n",
    "\\begin{bmatrix}\\sigma^2_1(m) & 0 & 0 \\\\ 0 & \\sigma^2_2(m)& 0  \\\\0 & 0 & \\sigma^2_3(m) \\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Our independent model for $f_i$ will be a Tanimoto-kernel GP with a constant mean $\\mu_i$,\n",
    "a kernel amplitude $a_i$, and a noise variance $s_i$.\n",
    "This is equivalent to modelling the _residual_ $(y-\\mu_i)$ with a _zero-mean_ GP\n",
    "with kernel $$k(x,x')=a_i T(x,x')$$ (where $T$ denotes Tanimoto similarity between fingerprints).\n",
    "The parameters $\\vec{a}, \\vec{\\mu}, \\vec{\\s}$ are the model hyperparameters.\n",
    "These can be tuned later.\n",
    "\n",
    "Because the covariance matrix is diagonal, we will make our model return predictions as a tuple of vectors\n",
    "\\begin{align*}\n",
    "    \\vec{\\mu}(m) &= \\begin{bmatrix} \\mu_1(m) & \\mu_2(m) & \\mu_3(m) \\end{bmatrix} \\\\\n",
    "    \\vec{\\sigma^2}(m) &= \\begin{bmatrix} \\sigma^2_1(m) & \\sigma^2_2(m) & \\sigma^2_3(m) \\end{bmatrix}\\ .\n",
    "\\end{align*}\n",
    "This is implemented in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4469ae81-b983-4118-aba4-26021277e0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fingerprint(smiles: str):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    assert mol is not None\n",
    "    return AllChem.GetMorganFingerprint(mol, radius=3, useCounts=True)\n",
    "\n",
    "\n",
    "def independent_tanimoto_gp_predict(\n",
    "    *,  # require inputting arguments by name\n",
    "    query_smiles: list[str],  # len M\n",
    "    known_smiles: list[str],  # len N\n",
    "    known_Y: np.ndarray,  # NxK\n",
    "    gp_means: np.ndarray,  # shape K\n",
    "    gp_amplitudes: np.ndarray,  # shape K\n",
    "    gp_noises: np.ndarray,  # shape K\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Make *independent* predictions on a set of query smiles with the\n",
    "    independent Tanimoto GP model.\n",
    "    \n",
    "    Return two arrays A and B with shape (M,K)\n",
    "    such that A[i,j] is the predicted mean for query_smiles[i]\n",
    "    on objective j\n",
    "    and B[i,j] is the predicted variance for query_smiles[i]\n",
    "    on objective j.\n",
    "\n",
    "    gp_means, gp_amplitudes, and gp_noises\n",
    "    are the model hyperparameters.\n",
    "\n",
    "    NOTE: this method can likely be made much more efficient if covariance matrices are cached\n",
    "    (i.e. calculated once and then passed in). If you change this in the future, this method\n",
    "    could be a helpful reference.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check that dimension of hyperparameters is correct\n",
    "    for hparam_arr in (gp_means, gp_amplitudes, gp_noises):\n",
    "        assert hparam_arr.shape == (known_Y.shape[1], )\n",
    "\n",
    "    # Create kernel matrices of Tanimoto similarities.\n",
    "    # These are shared between all 3 models\n",
    "    # NOTE: if you are calling this function many times on the same query/known smiles\n",
    "    # you could potentially cache this computation, but for now we won't worry about this.\n",
    "    known_fp = [get_fingerprint(s) for s in known_smiles]\n",
    "    query_fp = [get_fingerprint(s) for s in query_smiles]\n",
    "    K_known_known = np.asarray([DataStructs.BulkTanimotoSimilarity(fp, known_fp) for fp in known_fp])  # shape (N,N)\n",
    "    K_query_known = np.asarray([DataStructs.BulkTanimotoSimilarity(fp, known_fp) for fp in query_fp])  # shape (M,N)\n",
    "\n",
    "    # Compute DIAGNONAL of query-query covariance matrix. Don't need the full matrix since we are not\n",
    "    # making correlated predictions.\n",
    "    K_query_query_diagonal = np.asarray([DataStructs.TanimotoSimilarity(fp, fp) for fp in query_fp])\n",
    "\n",
    "    # Make separate predictions for each model.\n",
    "    # NOTE: this will invert the covariance matrix K times, and will therefore be slower than it could be.\n",
    "    # This is a design limitation of kern_GP and could be improved in the future.\n",
    "    means_out = []\n",
    "    vars_out = []\n",
    "    for j in range(known_Y.shape[1]):  # iterate over all objectives\n",
    "        residual_j = known_Y[:, j] - gp_means[j]\n",
    "        mu_j, var_j = kern_gp.noiseless_predict(\n",
    "            a=gp_amplitudes[j],\n",
    "            s=gp_noises[j],\n",
    "            k_train_train=K_known_known,\n",
    "            k_test_train=K_query_known,\n",
    "            k_test_test=K_query_query_diagonal,\n",
    "            y_train=residual_j,\n",
    "            full_covar=False\n",
    "        )\n",
    "        means_out.append(mu_j + gp_means[j])\n",
    "        vars_out.append(var_j)\n",
    "\n",
    "    # Return joint predictions\n",
    "    return (\n",
    "        np.asarray(means_out).T,\n",
    "        np.asarray(vars_out).T,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "974cf910-7904-43de-bf41-d369aa95d3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tanimoto Similarity: 0.5\n",
      "Mean and variance are as expected\n"
     ]
    }
   ],
   "source": [
    "# Test this model on two molecules with Tanimoto similarity = 0.5\n",
    "m1 = \"CCC\"; m2 = \"CCCC\"\n",
    "print(f\"Tanimoto Similarity: {DataStructs.TanimotoSimilarity(get_fingerprint(m1), get_fingerprint(m2)):}\")\n",
    "mu_pred, var_pred = independent_tanimoto_gp_predict(\n",
    "    query_smiles=[m2],\n",
    "    known_smiles=[m1],\n",
    "    known_Y=np.asarray([[1.0, 0.0, -1.0]]),\n",
    "    gp_means=np.asarray([0.0, 0.0, 1.0]),\n",
    "    gp_amplitudes=np.asarray([1.0, 0.5, 1.0]),\n",
    "    gp_noises=np.asarray([1.0, 1e-4, 1e-1]),\n",
    ")\n",
    "\n",
    "# Check that it is close to some manually-calculated values\n",
    "assert np.allclose(mu_pred, np.asarray([[1/4, 0, 1-1/1.1]]))\n",
    "assert np.allclose(var_pred, np.asarray([[1-0.5**3, 0.5*(1-0.5**2), 1-(0.5**2)/1.1]]), atol=1e-3)\n",
    "print(\"Mean and variance are as expected\")\n",
    "del mu_pred, var_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d758c1d-7bad-4349-9e14-a3e4fc818f4a",
   "metadata": {},
   "source": [
    "## Acquisition function\n",
    "\n",
    "We have written a function which returns the predicted mean and variance for all objectives.\n",
    "We will now write a separate function which computes an acquisition function from\n",
    "a set of predicted means and variances.\n",
    "Designing the acquisition function will likely be the most difficult part of the project.\n",
    "\n",
    "As a demonstration, we will use a simple acquisition function which I have made-up/reinvented: the probability of _any_ improvement (PAI).\n",
    "This is the probability that an observed value will improve on the best known value for at least 1 objective:\n",
    "\n",
    "$$\\mathrm{PAI}(x) = \\mathbb{P}_{\\vec{y}\\sim \\mathrm{model}(x)}\\left[i : y_i \\geq y_{best,i} \\right]$$\n",
    "\n",
    "This function can be calculated in two ways:\n",
    "1. Analytically: because the predictions are independent, PAI(x) = 1 - (probability that no $y_i$ improves)\n",
    "2. Monte Carlo: sample a bunch of $y$ values and check how many times improvement is observed\n",
    "\n",
    "We will code both below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee791375-4e45-4f61-8c08-d0d973add996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PAI_analytic(\n",
    "    *,\n",
    "    mu_pred: np.ndarray,  # shape (K,)\n",
    "    var_pred: np.ndarray,  # shape (K,)\n",
    "    y_best: np.ndarray,  # shape (K, )\n",
    ") -> float:\n",
    "\n",
    "    assert mu_pred.shape == var_pred.shape == y_best.shape\n",
    "\n",
    "    # The probability of a Gaussian variable being <= a certain value is given by its CDF.\n",
    "    # This is exactly the probability of not improving\n",
    "    prob_no_improve = stats.norm.cdf(y_best, loc=mu_pred, scale=np.sqrt(var_pred))\n",
    "\n",
    "    return float(1 - np.prod(prob_no_improve))\n",
    "\n",
    "def PAI_mc(\n",
    "    *,\n",
    "    mu_pred: np.ndarray,  # shape (K,)\n",
    "    var_pred: np.ndarray,  # shape (K,)\n",
    "    y_best: np.ndarray,  # shape (K, )\n",
    "    num_mc_samples: int,\n",
    ") -> float:\n",
    "\n",
    "    samples = stats.norm(loc=mu_pred, scale=np.sqrt(var_pred)).rvs(size=(num_mc_samples, len(y_best)))\n",
    "    return np.mean(np.any(samples > y_best, axis=1).astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d912719-11ca-40ae-88d3-40e96f26107e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytic: 0.29213901826285904\n",
      "MC: 0.309\n"
     ]
    }
   ],
   "source": [
    "# Test the acquisition function.\n",
    "# Values should be reasonably close\n",
    "mu_pred = np.asarray([1.0, 2.0])\n",
    "var_pred = np.asarray([0.5, 2.0])**2\n",
    "y_best = np.asarray([1.5, 4.0])\n",
    "print(f\"Analytic: {PAI_analytic(mu_pred=mu_pred, var_pred=var_pred, y_best=y_best)}\")\n",
    "print(f\"MC: {PAI_mc(mu_pred=mu_pred, var_pred=var_pred, y_best=y_best, num_mc_samples=1000)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16198aa2-4e34-4877-908f-c1d0ef347fd6",
   "metadata": {},
   "source": [
    "## BO loop\n",
    "\n",
    "In each iteration, a BO loop will:\n",
    "\n",
    "1. Fit the model\n",
    "2. Evaluate the acquisition function over all candidate SMILES\n",
    "3. Pick the SMILES with the highest acquisition function value\n",
    "4. Evaluate that SMILES and add it to the dataset\n",
    "\n",
    "Here we will write a very simple BO loop that does this.\n",
    "\n",
    "Note that writing this loop requires setting model hyperparameters.\n",
    "A good setting for the $\\mu$ hyperparameter is the dataset mean,\n",
    "while a good setting for the $a$ is the dataset variance (or potentially a higher value to encourage exploration).\n",
    "These are _not_ known in real life (and should be tuned),\n",
    "but for the purposes of this example we will calculate them on the real dataset\n",
    "and set them accordingly.\n",
    "Since these objectives are all noiseless, we will set the noise to a small value.\n",
    "\n",
    "Running the BO loop below should yield a small amount of improvement to the objectives.\n",
    "Most likely better tuning of hyperparameters will yield better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74553374-7687-4e0f-b7bd-f06158da9e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: [8.6079508  0.61403957 0.17458681]\n",
      "Var: [1.24827062 0.03021591 0.00322067]\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "all_Y_values = evaluate_objectives(ALL_SMILES)  # NOTE: is cheating, if we can just evaluate everything we wouldn't do BO\n",
    "\n",
    "# NOTE: in evaluation, disregard NaN values\n",
    "print(f\"Mean: {np.nanmean(all_Y_values, axis=0)}\")\n",
    "print(f\"Var: {np.nanvar(all_Y_values, axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8c9e3b5-8da9-4d3a-a23e-0f04a41eb7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start BO iter 0. Dataset size=(10, 3). Y_best=[11.5         0.90345663  0.20134228]\n",
      "\tMax acquisition value: 0.659\n",
      "\tSMILES 1 with acq fn = 0.659 is already known, skipping\n",
      "\tChose SMILES 6342 with acq fn = 0.647\n",
      "\tValue of chosen SMILES: [[7.5        0.73403691 0.14814815]]\n",
      "Start BO iter 1. Dataset size=(11, 3). Y_best=[11.5         0.90345663  0.20134228]\n",
      "\tMax acquisition value: 0.659\n",
      "\tSMILES 1 with acq fn = 0.659 is already known, skipping\n",
      "\tChose SMILES 7200 with acq fn = 0.645\n",
      "\tValue of chosen SMILES: [[8.6        0.7205121  0.17307692]]\n",
      "Start BO iter 2. Dataset size=(12, 3). Y_best=[11.5         0.90345663  0.20134228]\n",
      "\tMax acquisition value: 0.659\n",
      "\tSMILES 1 with acq fn = 0.659 is already known, skipping\n",
      "\tChose SMILES 1243 with acq fn = 0.645\n",
      "\tValue of chosen SMILES: [[6.3        0.24890958 0.00793651]]\n",
      "Start BO iter 3. Dataset size=(13, 3). Y_best=[11.5         0.90345663  0.20134228]\n",
      "\tMax acquisition value: 0.659\n",
      "\tSMILES 1 with acq fn = 0.659 is already known, skipping\n",
      "\tChose SMILES 2487 with acq fn = 0.645\n",
      "\tValue of chosen SMILES: [[7.1        0.82803953 0.14285714]]\n",
      "Start BO iter 4. Dataset size=(14, 3). Y_best=[11.5         0.90345663  0.20134228]\n",
      "\tMax acquisition value: 0.662\n",
      "\tSMILES 1 with acq fn = 0.662 is already known, skipping\n",
      "\tChose SMILES 1170 with acq fn = 0.648\n",
      "\tValue of chosen SMILES: [[6.4        0.75568154 0.23595506]]\n",
      "Start BO iter 5. Dataset size=(15, 3). Y_best=[11.5         0.90345663  0.23595506]\n",
      "\tMax acquisition value: 0.63\n",
      "\tChose SMILES 1276 with acq fn = 0.63\n",
      "\tValue of chosen SMILES: [[8.9        0.57093428 0.23853211]]\n",
      "Start BO iter 6. Dataset size=(16, 3). Y_best=[11.5         0.90345663  0.23853211]\n",
      "\tMax acquisition value: 0.627\n",
      "\tChose SMILES 9940 with acq fn = 0.627\n",
      "\tValue of chosen SMILES: [[7.2        0.85248428 0.19047619]]\n",
      "Start BO iter 7. Dataset size=(17, 3). Y_best=[11.5         0.90345663  0.23853211]\n",
      "\tMax acquisition value: 0.636\n",
      "\tChose SMILES 5448 with acq fn = 0.636\n",
      "\tValue of chosen SMILES: [[7.         0.85802428 0.15686275]]\n",
      "Start BO iter 8. Dataset size=(18, 3). Y_best=[11.5         0.90345663  0.23853211]\n",
      "\tMax acquisition value: 0.635\n",
      "\tChose SMILES 5970 with acq fn = 0.635\n",
      "\tValue of chosen SMILES: [[7.5        0.92014455 0.19047619]]\n",
      "Start BO iter 9. Dataset size=(19, 3). Y_best=[11.5         0.92014455  0.23853211]\n",
      "\tMax acquisition value: 0.656\n",
      "\tChose SMILES 9013 with acq fn = 0.656\n",
      "\tValue of chosen SMILES: [[7.5        0.81204109 0.18518519]]\n"
     ]
    }
   ],
   "source": [
    "# BO loop\n",
    "BO_known_smiles = list(known_smiles)\n",
    "BO_known_Y = known_Y.copy()\n",
    "for bo_iter in range(10):\n",
    "    y_best = np.max(BO_known_Y, axis=0)  # best eval so far\n",
    "    print(f\"Start BO iter {bo_iter}. Dataset size={BO_known_Y.shape}. Y_best={y_best}\")\n",
    "\n",
    "    # Make predictions\n",
    "    mu_pred, var_pred = independent_tanimoto_gp_predict(\n",
    "        query_smiles=ALL_SMILES,\n",
    "        known_smiles=BO_known_smiles,\n",
    "        known_Y=BO_known_Y,\n",
    "        gp_means=np.asarray([9., 0.6, 0.2]),  # Chosen from above\n",
    "        gp_amplitudes=np.asarray([2.0, 0.25, 0.25]),  # Chosen higher than actual means/vars above\n",
    "        gp_noises=np.asarray([1e-1, 1e-2, 1e-2]),  # small values\n",
    "    )\n",
    "\n",
    "    # Evaluate acquisition function (use analytic version)\n",
    "    # NOTE: can replace with other acquisition function later\n",
    "    acq_fn_values = [\n",
    "        PAI_analytic(mu_pred=m, var_pred=v, y_best=y_best)\n",
    "        for m, v in zip(mu_pred, var_pred)\n",
    "    ]\n",
    "    print(f\"\\tMax acquisition value: {max(acq_fn_values):.3g}\")\n",
    "\n",
    "    # Which SMILES maximizes the acquisition function value?\n",
    "    # be sure to choose a SMILES which was not chosen before!\n",
    "    for chosen_i in np.argsort(-np.asarray(acq_fn_values)):\n",
    "        if ALL_SMILES[chosen_i] in BO_known_smiles:\n",
    "            print(f\"\\tSMILES {chosen_i} with acq fn = {acq_fn_values[chosen_i]:.3g} is already known, skipping\")\n",
    "        else:\n",
    "            break\n",
    "    print(f\"\\tChose SMILES {chosen_i} with acq fn = {acq_fn_values[chosen_i]:.3g}\")\n",
    "\n",
    "    # Evaluate SMILES\n",
    "    chosen_smiles = ALL_SMILES[chosen_i]\n",
    "    new_y = evaluate_objectives([chosen_smiles])\n",
    "    assert not np.any(np.isnan(new_y)), \"NaN value detected in objective. Need to handle this case separately\"\n",
    "    print(f\"\\tValue of chosen SMILES: {new_y}\")\n",
    "\n",
    "    # Add to dataset\n",
    "    BO_known_smiles = BO_known_smiles + [chosen_smiles]\n",
    "    BO_known_Y = np.concatenate([BO_known_Y, new_y], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c36c2e-d9f3-494f-befe-7f7043f7db6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
